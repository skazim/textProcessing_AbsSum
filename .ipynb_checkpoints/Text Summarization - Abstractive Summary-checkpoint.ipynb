{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00dfdd7c",
   "metadata": {},
   "source": [
    "# Text Processing - Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d964f0a",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6763fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from contractions import contractions_dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Dense, Embedding, Bidirectional, TimeDistributed, Lambda, Attention, Concatenate\n",
    ")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5189655c",
   "metadata": {},
   "source": [
    "### Custom Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "049646dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(df1,df2):\n",
    "    df1Cols = df1.columns.tolist()\n",
    "    df1Cols.remove('headlines')\n",
    "    df1Cols.remove('text')\n",
    "    df1.drop(df1Cols, axis='columns', inplace=True)\n",
    "\n",
    "    df = pd.concat([df1, df2], axis='rows')\n",
    "    del df1, df2\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df.text = df.text.apply(str.lower)\n",
    "    df.headlines = df.headlines.apply(str.lower)\n",
    "    return df\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_map=contractions_dict):\n",
    "    # Using regex for getting all contracted words\n",
    "    contractions_keys = '|'.join(contraction_map.keys())\n",
    "    contractions_pattern = re.compile(f'({contractions_keys})', flags=re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        # Getting entire matched sub-string\n",
    "        match = contraction.group(0)\n",
    "        expanded_contraction = contraction_map.get(match)\n",
    "        if not expand_contractions:\n",
    "            print(match)\n",
    "            return match\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e4745",
   "metadata": {},
   "source": [
    "### Data Extraction/Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7acd8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    filename1 = 'news_summary.csv'\n",
    "    filename2 = 'news_summary_more.csv'\n",
    "    df1 = pd.read_csv(filename1, encoding='iso-8859-1').reset_index(drop=True)\n",
    "    df2 = pd.read_csv(filename2, encoding='iso-8859-1').reset_index(drop=True)\n",
    "    \n",
    "    df = data_preprocess(df1,df2)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4287fd7",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9729fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizingText(texts,max_len=100): # breaking text into smaller words or subwords\n",
    "    tokenizer = Tokenizer() \n",
    "    tokenizer.fit_on_texts(texts) # create vocabulary index based on word frequency if tokenize none\n",
    "    seq = tokenizer.texts_to_sequences(texts)\n",
    "    padded_seq = pad_sequences(seq,maxlen=max_len,padding='post')\n",
    "    return padded_seq,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cb5d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "ctext = df['text']\n",
    "headlines = df['headlines']\n",
    "\n",
    "maxlen = 100\n",
    "padded_input_seq, in_tokenizer = tokenizingText(ctext, max_len=maxlen)\n",
    "padded_target_seq, tar_tokenizer = tokenizingText(headlines, max_len=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d81eb7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using teacher forcing as a method for training RNN ( encode/decoder)\n",
    "decode_padded_input_seq = padded_target_seq[:, :-1]\n",
    "decode_padded_target_seq = padded_target_seq[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9527ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101749\n",
      "45129\n"
     ]
    }
   ],
   "source": [
    "print(len(in_tokenizer.word_index))\n",
    "print(len(tar_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08e86b",
   "metadata": {},
   "source": [
    "### Building a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "390e078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class EmbeddedLayer(keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        max_tokens = 100\n",
    "        return tf.one_hot(x, depth=max_tokens)\n",
    "\n",
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_out, decoder_out = inputs\n",
    "        attention = tf.reduce_sum(encoder_out * decoder_out, axis=-1)\n",
    "        return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e68674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_attention(inputs):\n",
    "#     encoder_outputs, decoder_outputs = inputs\n",
    "#     attention = Attention()([decoder_outputs, encoder_outputs])\n",
    "#     context_vector = tf.matmul(attention, encoder_outputs) \n",
    "#     return context_vector\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Defining the Model Architecture\n",
    "Builds a Seq2Seq model using work embeddings\n",
    "\n",
    "Parameters: \n",
    "in_vocab_size : size of input vocabs\n",
    "out_vocab_size: size of output vocabs\n",
    "emb_dimension : Dimentsion of the word embeddings\n",
    "units: no of units in RNN model\n",
    "\n",
    "Implements:\n",
    "\n",
    "-> Encoder :  \n",
    "-> Decoder :\n",
    "-> Attention :\n",
    "-> Output layer : \n",
    "-> Model\n",
    "\n",
    "output: keras model for training\n",
    "\"\"\"\n",
    "\n",
    "def create_model(in_vocab_size, out_vocab_size, emb_dimension, units):\n",
    "    \n",
    "    \"\"\"Encoder\"\"\"\n",
    "    encoder_in = Input(shape=(None,))\n",
    "#     encoder_in = EmbeddedLayer()(inputs)\n",
    "    encoder_emb = Embedding(in_vocab_size,emb_dimension)(encoder_in)\n",
    "    encoder_lstm = Bidirectional(LSTM(units, return_sequences=True, return_state=True))\n",
    "    encoder_out, forward_hidden, forward_cell, backward_hidden, backward_cell = encoder_lstm(encoder_emb)\n",
    "\n",
    "    state_hidden = Lambda(lambda x: tf.concat([x[0], x[1]], axis=-1))([forward_hidden, backward_hidden])\n",
    "    state_cell = Lambda(lambda x: tf.concat([x[0], x[1]], axis=-1))([forward_cell, backward_cell])\n",
    "\n",
    "    \"\"\"Attention\"\"\"\n",
    "    attention = AttentionLayer()([encoder_in, encoder_in])\n",
    "    \n",
    "    \n",
    "    \"\"\"Decoder\"\"\"\n",
    "    decoder_in = Input(shape=(None,), dtype=\"int64\")\n",
    "#     decoder_in = EmbeddedLayer()(inputs)\n",
    "    decoder_emb = Embedding(out_vocab_size, emb_dimension)(decoder_in)\n",
    "    decoder_lstm = LSTM(units * 2, return_sequences=True, return_state=True)\n",
    "    decoder_out, _, _ = decoder_lstm(decoder_emb, initial_state=[state_hidden, state_cell])\n",
    "\n",
    "#     attn_out = tf.concat([decoder_out, attention], axis=-1)\n",
    "#     attn_out = Concatenate(axis=-1)([decoder_out, attention])\n",
    "    \n",
    "    \"\"\"Output Layer\"\"\"\n",
    "    decoder_dense_layer = TimeDistributed(Dense(out_vocab_size, activation='softmax'))\n",
    "    outputs = decoder_dense_layer(decoder_out)\n",
    "\n",
    "    \"\"\"Building a model\"\"\"\n",
    "    model = Model([encoder_in, decoder_in], outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2f6c44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 18:07:47.834348: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 139/1287\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:47:53\u001b[0m 9s/step - accuracy: 0.8749 - loss: 3.8302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb_dimension = 256\n",
    "units = 128\n",
    "in_vocab_size = len(in_tokenizer.word_index) + 1\n",
    "out_vocab_size = len(tar_tokenizer.word_index) + 1\n",
    "\n",
    "model = create_model(in_vocab_size, out_vocab_size, emb_dimension,units)\n",
    "model.fit(\n",
    "    [padded_input_seq, decode_padded_input_seq],\n",
    "    np.expand_dims(decode_padded_target_seq, -1),\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f8d97",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a470d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, padded_input_seq, decode_padded_input_seq,\n",
    "#                    decode_padded_target_seq, tar_tokenizer, maxlen):\n",
    "def evaluate_model(model, input_encoder, output_decoder, target_decode, tokenizer, maxlen):\n",
    "    predictions = model.predict([input_encoder, output_decoder])\n",
    "    predicted_texts = []\n",
    "    for pred_seq in np.argmax(predictions, axis=-1):\n",
    "        predicted_texts.append(' '.join([tokenizer.index_word[idx] for idx in pred_seq if idx != 0]))\n",
    "    reference_texts = [' '.join([tokenizer.index_word[idx] for idx in seq if idx != 0]) for seq in target_decode]\n",
    "\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(predicted_texts, reference_texts, avg=True)\n",
    "    bleu_scores = [sentence_bleu([ref.split()], pred.split()) for ref, pred in zip(reference_texts, predicted_texts)]\n",
    "    return rouge_scores, bleu_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd00639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36927f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
